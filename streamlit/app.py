# streamlit/app.py
# streamlit/app.py
import streamlit as st
import os
from pathlib import Path
import shutil
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
from rag_core import build_and_load_knowledge_base, create_rag_chain, _load_api_key_from_env, hybrid_search_with_rerank
import rag_core as rc

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(
    page_title="RAG-—á–∞—Ç —Å –≤–∞—à–∏–º–∏ PDF",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ RAG-—á–∞—Ç —Å –≤–∞—à–∏–º–∏ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
st.markdown("""
–≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∫ –≤–∞—à–∏–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º PDF-–¥–æ–∫—É–º–µ–Ω—Ç–∞–º.
**–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1.  **–í–≤–µ–¥–∏—Ç–µ –≤–∞—à OpenRouter API –∫–ª—é—á** –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏.
2.  **–ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ PDF-—Ñ–∞–π–ª–æ–≤**.
3.  –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É **"–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"**, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
4.  **–ó–∞–¥–∞–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å** –∏ –ø–æ–ª—É—á–∏—Ç–µ –æ—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ –≤–∞—à–∏—Ö —Ñ–∞–π–ª–æ–≤.
""")

# --- –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å ---
with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    # –ö–æ–¥ –¥–ª—è API-–∫–ª—é—á–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á –∏–∑ .env, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—ã—à–µ
        from rag_core import _load_api_key_from_env
        default_key = _load_api_key_from_env()
    except (RuntimeError, ImportError):
        default_key = ""

    api_key_input = st.text_input(
        "OpenRouter API Key",
        type="password",
        placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á...",
        value=default_key,
        help="–í—ã –º–æ–∂–µ—Ç–µ –ø–æ–ª—É—á–∏—Ç—å –∫–ª—é—á –Ω–∞ openrouter.ai"
    )
    if api_key_input:
        st.session_state.openrouter_api_key = api_key_input
    
    st.divider()

    uploaded_files = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–∏ PDF-—Ñ–∞–π–ª—ã",
        type="pdf",
        accept_multiple_files=True
    )
    process_button = st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã", type="primary")
    st.divider()
    lang_filter_flag = st.checkbox("Language filter ON", value=True, help="–í–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —è–∑—ã–∫—É (SAME_LANG_RATIO)")

    # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å Recall level: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–π efSearch –Ω–∞ —Å–µ—Å—Å–∏—é
    recall_options = {
        "Fast (ef=64)": 64,
        "Balanced (ef=128)": 128,
        "High (ef=256)": 256,
        "Max (ef=384)": 384,
    }
    recall_labels = list(recall_options.keys())
    default_recall_label = st.session_state.get("recall_label", "Balanced (ef=128)")
    try:
        default_idx = recall_labels.index(default_recall_label)
    except ValueError:
        default_idx = 1
    recall_label = st.selectbox("Recall level", recall_labels, index=default_idx, help="–ö–æ–Ω—Ç—Ä–æ–ª—å —Å–∫–æ—Ä–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏: –±–∞–∑–æ–≤—ã–π efSearch –¥–ª—è HNSW")
    st.session_state.recall_label = recall_label
    rc.HNSW_EF_SEARCH_BASE = int(recall_options.get(recall_label, 128))

    st.divider()
    st.subheader("MMR –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç")
    mmr_lambda = st.slider("MMR_LAMBDA", min_value=0.2, max_value=0.8, value=float(getattr(rc, "MMR_LAMBDA", 0.5)), step=0.05, help="–ë–∞–ª–∞–Ω—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å/–¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∏ MMR")
    rc.MMR_LAMBDA = float(mmr_lambda)

    per_doc_cap = st.number_input("PER_DOC_CAP", min_value=1, max_value=10, value=int(getattr(rc, "PER_DOC_CAP", 2)), step=1, help="–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—É—Å–∫–æ–≤ —Å –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
    rc.PER_DOC_CAP = int(per_doc_cap)

    per_page_cap = st.number_input("PER_PAGE_CAP", min_value=0, max_value=10, value=int(getattr(rc, "PER_PAGE_CAP", 1)), step=1, help="–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—É—Å–∫–æ–≤ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (0 = –Ω–µ—Ç –ª–∏–º–∏—Ç–∞)")
    rc.PER_PAGE_CAP = int(per_page_cap)

    st.markdown("LANG_MIN_COVER (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∫–≤–æ—Ç—ã)")
    _ru_default = bool((getattr(rc, "LANG_MIN_COVER", {"ru":1,"en":1}).get("ru", 1)) > 0)
    _en_default = bool((getattr(rc, "LANG_MIN_COVER", {"ru":1,"en":1}).get("en", 1)) > 0)
    lang_ru_checked = st.checkbox("RU", value=_ru_default)
    lang_en_checked = st.checkbox("EN", value=_en_default)
    rc.LANG_MIN_COVER = {"ru": (1 if lang_ru_checked else 0), "en": (1 if lang_en_checked else 0)}

    enable_mmr = st.checkbox("Enable MMR", value=st.session_state.get("enable_mmr", True), help="–û—Ç–∫–ª—é—á–∏—Ç—å MMR –¥–ª—è A/B —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å top-K –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞")
    st.session_state.enable_mmr = bool(enable_mmr)

    st.divider()
    st.subheader("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è")
    model_options_str = os.getenv("RAG_UI_MODEL_OPTIONS", "")
    default_model_value = getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")
    model_options = [s.strip() for s in model_options_str.split(",") if s.strip()] or [default_model_value]
    default_model_sel = st.session_state.get("llm_model", default_model_value)
    if default_model_sel not in model_options:
        model_options = [default_model_sel] + [m for m in model_options if m != default_model_sel]
    llm_model = st.selectbox("LLM_MODEL", model_options, index=0)
    st.session_state.llm_model = llm_model

    llm_max_tokens = st.number_input(
        "LLM_MAX_TOKENS",
        min_value=200,
        max_value=4000,
        value=int(st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550)))),
        step=50,
    )
    st.session_state.llm_max_tokens = int(llm_max_tokens)

    llm_temperature = st.slider(
        "LLM_TEMPERATURE",
        min_value=0.0,
        max_value=1.0,
        value=float(st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2)))),
        step=0.05,
    )
    st.session_state.llm_temperature = float(llm_temperature)

    enforce_citations = st.checkbox(
        "Enforce citations",
        value=bool(st.session_state.get("enforce_citations", True)),
        help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è [S#] —Å –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–π –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π",
    )
    st.session_state.enforce_citations = bool(enforce_citations)

    language_enforcement = st.checkbox(
        "Language enforcement",
        value=bool(st.session_state.get("language_enforcement", True)),
        help="–ñ—ë—Å—Ç–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å —è–∑—ã–∫–∞ –æ—Ç–≤–µ—Ç–∞ –∏ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–∏",
    )
    st.session_state.language_enforcement = bool(language_enforcement)

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---
# –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞ (–∫–æ—Ä–µ–Ω—å), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–µ–∫—É—â–µ–π —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
BASE_DIR = Path(__file__).resolve().parents[1]
PDF_DIR = BASE_DIR / "pdfs"
PDF_DIR.mkdir(parents=True, exist_ok=True)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏ ---
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–æ–ø–∫–∏ "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã" ---
if process_button:
    if not uploaded_files:
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω PDF-—Ñ–∞–π–ª.")
    elif 'openrouter_api_key' not in st.session_state or not st.session_state.openrouter_api_key:
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à OpenRouter API –∫–ª—é—á –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏.")
    else:
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö
        for file_path in PDF_DIR.iterdir():
            if file_path.is_file():
                file_path.unlink()
        for uploaded_file in uploaded_files:
            with open(PDF_DIR / uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
        
        with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è."):
            try:
                build_and_load_knowledge_base(
                    pdf_dir=PDF_DIR,
                    index_dir=rc.VECTOR_STORE_PATH,
                    force_rebuild=True
                )
                st.session_state.rag_chain = create_rag_chain(
                    st.session_state.openrouter_api_key,
                    openrouter_model=st.session_state.get("llm_model", getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")),
                    temperature=st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2))),
                    max_tokens=st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550))),
                    enforce_citations=st.session_state.get("enforce_citations", True),
                    language_enforcement=st.session_state.get("language_enforcement", True),
                )
                st.session_state.messages = []
                st.success("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
                if Path(rc.VECTOR_STORE_PATH).exists():
                    shutil.rmtree(Path(rc.VECTOR_STORE_PATH))

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ, –µ—Å–ª–∏ –æ–Ω–∞ —É–∂–µ –µ—Å—Ç—å ---
if st.session_state.rag_chain is None and 'openrouter_api_key' in st.session_state:
    try:
        if build_and_load_knowledge_base(
            pdf_dir=PDF_DIR,
            index_dir=rc.VECTOR_STORE_PATH,
            force_rebuild=False
        ):
            st.session_state.rag_chain = create_rag_chain(
                st.session_state.openrouter_api_key,
                openrouter_model=st.session_state.get("llm_model", getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")),
                temperature=st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2))),
                max_tokens=st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550))),
                enforce_citations=st.session_state.get("enforce_citations", True),
                language_enforcement=st.session_state.get("language_enforcement", True),
            )
            st.toast("‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞.", icon="üìö")
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {e}. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ —Ñ–∞–π–ª—ã –∑–∞–Ω–æ–≤–æ.")

# --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∞—Ç–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ ---
st.header("–î–∏–∞–ª–æ–≥")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "context" in message and message["context"]:
             with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –≤ LLM"):
                st.text(message["context"])

if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å..."):
    if st.session_state.rag_chain is None:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–ª–∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ API –∫–ª—é—á –≤–≤–µ–¥–µ–Ω.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("–ò–¥—ë—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, —Ä–µ—Ä–∞–Ω–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
                try:
                    # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ –≤ —Ü–µ–ø–æ—á–∫—É —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏)
                    result = hybrid_search_with_rerank(prompt, apply_lang_quota=bool(lang_filter_flag))
                    fused = result.get("fused", [])
                    reranked = result.get("reranked", [])
                    context_pack = result.get("context_pack", [])
                    context_stats = result.get("context_stats", {})
                    q_lang = result.get("q_lang")
                    active_branches = result.get("active_branches")
                    sources_map = result.get("sources_map", {}) or {}

                    # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
                    st.markdown(f"**–°–≤–æ–¥–∫–∞:** {{'q_lang': '{q_lang}', 'active_branches': {active_branches}, 'fused': {len(fused)}, 'reranked': {len(reranked)}}}")

                    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —è–∫–æ—Ä—è –¥–ª—è –±—É–¥—É—â–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ PDF
                    def _make_anchor(item: dict) -> str:
                        """–°—Ç—Ä–æ–∏—Ç —è–∫–æ—Ä—å –≤–∏–¥–∞ ?file={source}&page={page} –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏."""
                        src = item.get("source")
                        pg = item.get("page")
                        if src and isinstance(pg, int):
                            return f"?file={src}&page={pg}"
                        return ""

                    # –ë–ª–æ–∫ RRF-—Å–ª–∏—è–Ω–∏–µ (top-20)
                    with st.expander("RRF-—Å–ª–∏—è–Ω–∏–µ (top-20)"):
                        rows_fused = [
                            {
                                "fusion_score": it.get("fusion_score"),
                                "min_rank": it.get("min_rank"),
                                "hits": it.get("hits"),
                                "source": it.get("source"),
                                "page": it.get("page"),
                                "citation": it.get("citation"),
                                "anchor": _make_anchor(it),
                            }
                            for it in fused[:20]
                        ]
                        st.dataframe(rows_fused, width='stretch')

                    # –ë–ª–æ–∫ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ä–∞–Ω–∫–∞ (top-5)
                    with st.expander("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ä–∞–Ω–∫–∞ (top-5)"):
                        rows_rerank = [
                            {
                                "rerank_score": it.get("rerank_score"),
                                "source": it.get("source"),
                                "page": it.get("page"),
                                "citation": it.get("citation"),
                                "retrieval_hits": it.get("hits"),
                                "anchor": _make_anchor(it),
                            }
                            for it in reranked[:5]
                        ]
                        st.dataframe(rows_rerank, width='stretch')

                    # –ü–∞–Ω–µ–ª—å Context Pack (–ø–æ—Å–ª–µ MMR)
                    with st.expander("Context Pack (–ø–æ—Å–ª–µ MMR)", expanded=True):
                        # –¢–∞–±–ª–∏—Ü–∞: source | page | citation | lang | reason_score | mmr_gain | dup_flags | tokens_est
                        def _tokens_est(meta_text: str | None, lang: str | None) -> int:
                            try:
                                from rag_core import _estimate_tokens
                                return int(_estimate_tokens(meta_text, lang))
                            except Exception:
                                return 0
                        from rag_core import chunks_metadata as _meta
                        rows_ctx = []
                        seen = set()
                        for it in context_pack:
                            cid = it.get("chunk_id")
                            if isinstance(cid, int) and 0 <= cid < len(_meta) and cid not in seen:
                                m = _meta[cid]
                                lang = m.get("lang")
                                anchor = _make_anchor({"source": m.get("source"), "page": m.get("page")})
                                rows_ctx.append({
                                    "source": m.get("source"),
                                    "page": m.get("page"),
                                    "citation": it.get("citation"),
                                    "lang": lang,
                                    "reason_score": it.get("rerank_score"),
                                    "mmr_gain": {
                                        "rel": it.get("mmr_rel"),
                                        "div": it.get("mmr_div"),
                                        "score": it.get("mmr_score"),
                                    },
                                    "dup_flags": None,
                                    "tokens_est": _tokens_est(m.get("text"), lang),
                                    "anchor": anchor,
                                })
                                seen.add(cid)
                        st.dataframe(rows_ctx, width='stretch')

                        # –°–≤–æ–¥–∫–∞
                        k = len(context_pack)
                        n = len(reranked)
                        budget_used = int(context_stats.get("budget_used_tokens", 0))
                        budget_limit = int(context_stats.get("budget_limit", 0))
                        lang_dist = context_stats.get("lang_distribution", {}) or {}
                        docs_dist = context_stats.get("doc_distribution", {}) or {}
                        st.markdown(f"–í–∑—è—Ç–æ {k} –∏–∑ {n}; –±—é–¥–∂–µ—Ç {budget_used}/{budget_limit}")
                        st.markdown(f"–Ø–∑—ã–∫–∏: {lang_dist}")
                        st.markdown(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {docs_dist}")

                        # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Ä–µ–∂–∏–º—ã –æ—Å–ª–∞–±–ª–µ–Ω–∏—è
                        thresholds = context_stats.get("thresholds", {}) or {}
                        with st.expander("–ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Ä–µ–∂–∏–º—ã –æ—Å–ª–∞–±–ª–µ–Ω–∏—è"):
                            st.json(thresholds)

                        # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (rejected reasons)
                        rejected_reasons = context_stats.get("rejected_reasons", {}) or {}
                        with st.expander("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã"):
                            rej_rows = [
                                {"reason": k, "count": v}
                                for k, v in sorted(rejected_reasons.items(), key=lambda x: (-int(x[1]), str(x[0])))
                            ]
                            st.dataframe(rej_rows, width='stretch')

                    # –û—Ç–ª–∞–¥–∫–∞ efSearch: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
                    with st.expander("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ efSearch"):
                        st.write(f"–ë–∞–∑–æ–≤—ã–π efSearch (—Å–µ—Å—Å–∏—è): {rc.HNSW_EF_SEARCH_BASE}")
                        st.caption("–ü—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–π ef –¥–ª—è –∫–∞–∂–¥–æ–π dense-–≤–µ—Ç–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ RAG_DEBUG=1 –≤ —Å–µ—Ä–≤–µ—Ä–Ω—ã–π –ª–æ–≥.")

                    # –ü–æ–∫–∞–∑–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –¥–æ–ª–∏ –ø–æ –≤–µ—Ç–∫–∞–º
                    with st.expander("–Ø–∑—ã–∫–æ–≤—ã–µ –¥–æ–ª–∏ –ø–æ –≤–µ—Ç–∫–∞–º"):
                        if active_branches:
                            from rag_core import chunks_metadata
                            def _share(items, target_lang):
                                if not items:
                                    return (0, 0, 0.0)
                                same = 0
                                for it in items:
                                    cid = it.get("chunk_id")
                                    if isinstance(cid, int) and 0 <= cid < len(chunks_metadata):
                                        if chunks_metadata[cid].get("lang") == target_lang:
                                            same += 1
                                other = len(items) - same
                                ratio = (same / len(items)) if len(items) else 0.0
                                return (same, other, ratio)
                        
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º reranked –∫–∞–∫ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ top-N
                        branch_map = {
                            "dense_en": ('en', [it for it in fused if 'dense_en' in (it.get('hits') or [])]),
                            "bm25_en": ('en', [it for it in fused if 'bm25_en' in (it.get('hits') or [])]),
                            "dense_ru": ('ru', [it for it in fused if 'dense_ru' in (it.get('hits') or [])]),
                            "bm25_ru": ('ru', [it for it in fused if 'bm25_ru' in (it.get('hits') or [])]),
                        }
                        for br in active_branches:
                            target_lang, items = branch_map.get(br, (None, []))
                            if not target_lang:
                                continue
                            same, other, share = _share(items, target_lang)
                            st.write(f"{br}: same_lang={same}, other_lang={other}, share={share:.2f}")

                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                    try:
                        gen_fn = st.session_state.rag_chain.get("answer_question") if isinstance(st.session_state.rag_chain, dict) else None
                        if callable(gen_fn):
                            gen_out = gen_fn(prompt, apply_lang_quota=bool(lang_filter_flag))
                        else:
                            gen_out = {"final_answer": "", "used_sources": [], "answer_lang_detected": None, "flags": {}}
                    except Exception as _e:
                        gen_out = {"final_answer": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {_e}", "used_sources": [], "answer_lang_detected": None, "flags": {}}

                    st.subheader("–û—Ç–≤–µ—Ç")
                    answer_text = gen_out.get("final_answer") or ""
                    st.markdown(answer_text)

                    used_labels = gen_out.get("used_sources", []) or []
                    from rag_core import chunks_metadata as _meta
                    rows_sources = []
                    for label in used_labels:
                        cid = sources_map.get(label)
                        if isinstance(cid, int) and 0 <= int(cid) < len(_meta):
                            m = _meta[int(cid)]
                            rows_sources.append({
                                "S#": label,
                                "source": m.get("source"),
                                "page": m.get("page"),
                                "anchor": _make_anchor({"source": m.get("source"), "page": m.get("page")}),
                            })
                    with st.expander("–°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", expanded=bool(rows_sources)):
                        st.dataframe(rows_sources, width='stretch')

                    flags = gen_out.get("flags", {}) or {}
                    lang_detected = gen_out.get("answer_lang_detected")
                    st.caption(f"–Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang_detected or 'unk'} | –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: lang={bool(flags.get('regenerated_for_lang'))}, citations={bool(flags.get('regenerated_for_citations'))}")

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"–°–≤–æ–¥–∫–∞: {{'fused': {len(fused)}, 'reranked': {len(reranked)}}}",
                    })
                except Exception as e:
                    st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ/—Å–ª–∏—è–Ω–∏–∏/—Ä–µ—Ä–∞–Ω–∫–µ: {e}")