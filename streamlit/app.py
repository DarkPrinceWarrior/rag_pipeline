# streamlit/app.py
# streamlit/app.py
import streamlit as st
import os
from pathlib import Path
import shutil
# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
from rag_core import build_and_load_knowledge_base, create_rag_chain, _load_api_key_from_env, hybrid_search_with_rerank
import rag_core as rc

# --- ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ ---
st.set_page_config(
    page_title="RAG-Ñ‡Ð°Ñ‚ Ñ Ð²Ð°ÑˆÐ¸Ð¼Ð¸ PDF",
    page_icon="ðŸ¤–",
    layout="wide"
)

st.title("ðŸ¤– RAG-Ñ‡Ð°Ñ‚ Ñ Ð²Ð°ÑˆÐ¸Ð¼Ð¸ PDF-Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸")
st.markdown("""
Ð­Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð²Ð°Ð¼ Ð·Ð°Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ðº Ð²Ð°ÑˆÐ¸Ð¼ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼ PDF-Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼.
**ÐšÐ°Ðº ÑÑ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚:**
1.  **Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð°Ñˆ OpenRouter API ÐºÐ»ÑŽÑ‡** Ð² Ð±Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿Ð°Ð½ÐµÐ»Ð¸.
2.  **Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð¾Ð´Ð¸Ð½ Ð¸Ð»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ PDF-Ñ„Ð°Ð¹Ð»Ð¾Ð²**.
3.  ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ÐºÐ½Ð¾Ð¿ÐºÑƒ **"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹"**, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½Ð¸Ð¹.
4.  **Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ ÑÐ²Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ** Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚Ðµ Ð¾Ñ‚Ð²ÐµÑ‚, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ð¸ Ð²Ð°ÑˆÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð².
""")

# --- Ð‘Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿Ð°Ð½ÐµÐ»ÑŒ ---
with st.sidebar:
    st.header("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸")
    # ÐšÐ¾Ð´ Ð´Ð»Ñ API-ÐºÐ»ÑŽÑ‡Ð° Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹...
    try:
        # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡ Ð¸Ð· .env, ÐµÑÐ»Ð¸ Ð¾Ð½ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð²Ñ‹ÑˆÐµ
        from rag_core import _load_api_key_from_env
        default_key = _load_api_key_from_env()
    except (RuntimeError, ImportError):
        default_key = ""

    api_key_input = st.text_input(
        "OpenRouter API Key",
        type="password",
        placeholder="Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð°Ñˆ ÐºÐ»ÑŽÑ‡...",
        value=default_key,
        help="Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡ Ð½Ð° openrouter.ai"
    )
    if api_key_input:
        st.session_state.openrouter_api_key = api_key_input
    
    st.divider()

    uploaded_files = st.file_uploader(
        "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð²Ð°ÑˆÐ¸ PDF-Ñ„Ð°Ð¹Ð»Ñ‹",
        type="pdf",
        accept_multiple_files=True
    )
    process_button = st.button("ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹", type="primary")
    st.divider()
    lang_filter_flag = st.checkbox("Language filter ON", value=True, help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ ÑÐ·Ñ‹ÐºÑƒ (SAME_LANG_RATIO)")

    # ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°Ñ‚ÐµÐ»ÑŒ Recall level: Ð¿ÐµÑ€ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ efSearch Ð½Ð° ÑÐµÑÑÐ¸ÑŽ
    recall_options = {
        "Fast (ef=64)": 64,
        "Balanced (ef=128)": 128,
        "High (ef=256)": 256,
        "Max (ef=384)": 384,
    }
    recall_labels = list(recall_options.keys())
    default_recall_label = st.session_state.get("recall_label", "Balanced (ef=128)")
    try:
        default_idx = recall_labels.index(default_recall_label)
    except ValueError:
        default_idx = 1
    recall_label = st.selectbox("Recall level", recall_labels, index=default_idx, help="ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸/Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸: Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ efSearch Ð´Ð»Ñ HNSW")
    st.session_state.recall_label = recall_label
    rc.HNSW_EF_SEARCH_BASE = int(recall_options.get(recall_label, 128))

    st.divider()
    st.subheader("MMR Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚")
    mmr_lambda = st.slider("MMR_LAMBDA", min_value=0.2, max_value=0.8, value=float(getattr(rc, "MMR_LAMBDA", 0.5)), step=0.05, help="Ð‘Ð°Ð»Ð°Ð½Ñ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ/Ð´Ð¸Ð²ÐµÑ€ÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸ MMR")
    rc.MMR_LAMBDA = float(mmr_lambda)

    per_doc_cap = st.number_input("PER_DOC_CAP", min_value=1, max_value=10, value=int(getattr(rc, "PER_DOC_CAP", 2)), step=1, help="ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð½Ð° ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÑƒÑÐºÐ¾Ð² Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°")
    rc.PER_DOC_CAP = int(per_doc_cap)

    per_page_cap = st.number_input("PER_PAGE_CAP", min_value=0, max_value=10, value=int(getattr(rc, "PER_PAGE_CAP", 1)), step=1, help="ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð½Ð° ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÑƒÑÐºÐ¾Ð² Ñ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ (0 = Ð½ÐµÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚Ð°)")
    rc.PER_PAGE_CAP = int(per_page_cap)

    st.markdown("LANG_MIN_COVER (Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ²Ð¾Ñ‚Ñ‹)")
    _ru_default = bool((getattr(rc, "LANG_MIN_COVER", {"ru":1,"en":1}).get("ru", 1)) > 0)
    _en_default = bool((getattr(rc, "LANG_MIN_COVER", {"ru":1,"en":1}).get("en", 1)) > 0)
    lang_ru_checked = st.checkbox("RU", value=_ru_default)
    lang_en_checked = st.checkbox("EN", value=_en_default)
    rc.LANG_MIN_COVER = {"ru": (1 if lang_ru_checked else 0), "en": (1 if lang_en_checked else 0)}

    enable_mmr = st.checkbox("Enable MMR", value=st.session_state.get("enable_mmr", True), help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ MMR Ð´Ð»Ñ A/B ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ñ top-K Ð¿Ð¾ÑÐ»Ðµ Ñ€ÐµÑ€Ð°Ð½ÐºÐ°")
    st.session_state.enable_mmr = bool(enable_mmr)

    st.divider()
    st.subheader("Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ")
    model_options_str = os.getenv("RAG_UI_MODEL_OPTIONS", "")
    default_model_value = getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")
    model_options = [s.strip() for s in model_options_str.split(",") if s.strip()] or [default_model_value]
    default_model_sel = st.session_state.get("llm_model", default_model_value)
    if default_model_sel not in model_options:
        model_options = [default_model_sel] + [m for m in model_options if m != default_model_sel]
    llm_model = st.selectbox("LLM_MODEL", model_options, index=0)
    st.session_state.llm_model = llm_model

    llm_max_tokens = st.number_input(
        "LLM_MAX_TOKENS",
        min_value=200,
        max_value=4000,
        value=int(st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550)))),
        step=50,
    )
    st.session_state.llm_max_tokens = int(llm_max_tokens)

    llm_temperature = st.slider(
        "LLM_TEMPERATURE",
        min_value=0.0,
        max_value=1.0,
        value=float(st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2)))),
        step=0.05,
    )
    st.session_state.llm_temperature = float(llm_temperature)

    enforce_citations = st.checkbox(
        "Enforce citations",
        value=bool(st.session_state.get("enforce_citations", True)),
        help="ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ [S#] Ñ Ð¾Ð´Ð½Ð¾ÐºÑ€Ð°Ñ‚Ð½Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹",
    )
    st.session_state.enforce_citations = bool(enforce_citations)

    language_enforcement = st.checkbox(
        "Language enforcement",
        value=bool(st.session_state.get("language_enforcement", True)),
        help="Ð–Ñ‘ÑÑ‚ÐºÐ¸Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÑÐ·Ñ‹ÐºÐ° Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¸ Ð¿ÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸ Ð½Ð°Ñ€ÑƒÑˆÐµÐ½Ð¸Ð¸",
    )
    st.session_state.language_enforcement = bool(language_enforcement)

# --- ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° ---
# Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° (ÐºÐ¾Ñ€ÐµÐ½ÑŒ), Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
BASE_DIR = Path(__file__).resolve().parents[1]
PDF_DIR = BASE_DIR / "pdfs"
PDF_DIR.mkdir(parents=True, exist_ok=True)

# --- Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ÑÐµÑÑÐ¸Ð¸ ---
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ½Ð¾Ð¿ÐºÐ¸ "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹" ---
if process_button:
    if not uploaded_files:
        st.error("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð¸Ð½ PDF-Ñ„Ð°Ð¹Ð».")
    elif 'openrouter_api_key' not in st.session_state or not st.session_state.openrouter_api_key:
        st.error("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð°Ñˆ OpenRouter API ÐºÐ»ÑŽÑ‡ Ð² Ð±Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿Ð°Ð½ÐµÐ»Ð¸.")
    else:
        # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ñ‹Ñ…
        for file_path in PDF_DIR.iterdir():
            if file_path.is_file():
                file_path.unlink()
        for uploaded_file in uploaded_files:
            with open(PDF_DIR / uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
        
        with st.spinner("Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹... Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼Ñ."):
            try:
                build_and_load_knowledge_base(
                    pdf_dir=PDF_DIR,
                    index_dir=rc.VECTOR_STORE_PATH,
                    force_rebuild=True
                )
                st.session_state.rag_chain = create_rag_chain(
                    st.session_state.openrouter_api_key,
                    openrouter_model=st.session_state.get("llm_model", getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")),
                    temperature=st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2))),
                    max_tokens=st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550))),
                    enforce_citations=st.session_state.get("enforce_citations", True),
                    language_enforcement=st.session_state.get("language_enforcement", True),
                )
                st.session_state.messages = []
                st.success("Ð‘Ð°Ð·Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð°!")
            except Exception as e:
                st.error(f"ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {e}")
                if Path(rc.VECTOR_STORE_PATH).exists():
                    shutil.rmtree(Path(rc.VECTOR_STORE_PATH))

# --- Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ, ÐµÑÐ»Ð¸ Ð¾Ð½Ð° ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ ---
if st.session_state.rag_chain is None and 'openrouter_api_key' in st.session_state:
    try:
        if build_and_load_knowledge_base(
            pdf_dir=PDF_DIR,
            index_dir=rc.VECTOR_STORE_PATH,
            force_rebuild=False
        ):
            st.session_state.rag_chain = create_rag_chain(
                st.session_state.openrouter_api_key,
                openrouter_model=st.session_state.get("llm_model", getattr(rc, "OPENROUTER_MODEL", "openai/gpt-oss-120b")),
                temperature=st.session_state.get("llm_temperature", float(getattr(rc, "LLM_DEFAULT_TEMPERATURE", 0.2))),
                max_tokens=st.session_state.get("llm_max_tokens", int(getattr(rc, "LLM_DEFAULT_MAX_TOKENS", 550))),
                enforce_citations=st.session_state.get("enforce_citations", True),
                language_enforcement=st.session_state.get("language_enforcement", True),
            )
            st.toast("âœ… Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð°Ñ Ð±Ð°Ð·Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°.", icon="ðŸ“š")
    except Exception as e:
        st.warning(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½Ð¸Ð¹: {e}. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð¹Ñ‚Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð·Ð°Ð½Ð¾Ð²Ð¾.")

# --- ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð° Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² ---
st.header("Ð”Ð¸Ð°Ð»Ð¾Ð³")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "context" in message and message["context"]:
             with st.expander("ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚, Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ð¹ Ð² LLM"):
                st.text(message["context"])

if prompt := st.chat_input("Ð’Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ..."):
    if st.session_state.rag_chain is None:
        st.warning("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð¹Ñ‚Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¸Ð»Ð¸ ÑƒÐ±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ API ÐºÐ»ÑŽÑ‡ Ð²Ð²ÐµÐ´ÐµÐ½.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Ð˜Ð´Ñ‘Ñ‚ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº, Ñ€ÐµÑ€Ð°Ð½Ðº Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°..."):
                try:
                    # ÐŸÑ€Ð¾Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ Ñ„Ð»Ð°Ð³ Ð² Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÑƒ Ñ‡ÐµÑ€ÐµÐ· Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ (Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸)
                    result = hybrid_search_with_rerank(prompt, apply_lang_quota=bool(lang_filter_flag))
                    fused = result.get("fused", [])
                    reranked = result.get("reranked", [])
                    context_pack = result.get("context_pack", [])
                    context_stats = result.get("context_stats", {})
                    q_lang = result.get("q_lang")
                    active_branches = result.get("active_branches")
                    sources_map = result.get("sources_map", {}) or {}

                    # ÐšÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ°
                    st.markdown(f"**Ð¡Ð²Ð¾Ð´ÐºÐ°:** {{'q_lang': '{q_lang}', 'active_branches': {active_branches}, 'fused': {len(fused)}, 'reranked': {len(reranked)}}}")

                    # Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ ÑÐºÐ¾Ñ€Ñ Ð´Ð»Ñ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð° PDF
                    def _make_anchor(item: dict) -> str:
                        """Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ ÑÐºÐ¾Ñ€ÑŒ Ð²Ð¸Ð´Ð° ?file={source}&page={page} Ð´Ð»Ñ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸."""
                        src = item.get("source")
                        pg = item.get("page")
                        if src and isinstance(pg, int):
                            return f"?file={src}&page={pg}"
                        return ""

                    # Ð‘Ð»Ð¾Ðº RRF-ÑÐ»Ð¸ÑÐ½Ð¸Ðµ (top-20)
                    with st.expander("RRF-ÑÐ»Ð¸ÑÐ½Ð¸Ðµ (top-20)"):
                        rows_fused = [
                            {
                                "fusion_score": it.get("fusion_score"),
                                "min_rank": it.get("min_rank"),
                                "hits": it.get("hits"),
                                "source": it.get("source"),
                                "page": it.get("page"),
                                "citation": it.get("citation"),
                                "anchor": _make_anchor(it),
                            }
                            for it in fused[:20]
                        ]
                        st.dataframe(rows_fused, width='stretch')

                    # Ð‘Ð»Ð¾Ðº Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ€ÐµÑ€Ð°Ð½ÐºÐ° (top-5)
                    with st.expander("Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ€ÐµÑ€Ð°Ð½ÐºÐ° (top-5)"):
                        rows_rerank = [
                            {
                                "rerank_score": it.get("rerank_score"),
                                "source": it.get("source"),
                                "page": it.get("page"),
                                "citation": it.get("citation"),
                                "retrieval_hits": it.get("hits"),
                                "anchor": _make_anchor(it),
                            }
                            for it in reranked[:5]
                        ]
                        st.dataframe(rows_rerank, width='stretch')

                    # ÐŸÐ°Ð½ÐµÐ»ÑŒ Context Pack (Ð¿Ð¾ÑÐ»Ðµ MMR)
                    with st.expander("Context Pack (Ð¿Ð¾ÑÐ»Ðµ MMR)", expanded=True):
                        # Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð°: source | page | citation | lang | reason_score | mmr_gain | dup_flags | tokens_est
                        def _tokens_est(meta_text: str | None, lang: str | None) -> int:
                            try:
                                from rag_core import _estimate_tokens
                                return int(_estimate_tokens(meta_text, lang))
                            except Exception:
                                return 0
                        from rag_core import chunks_metadata as _meta
                        rows_ctx = []
                        seen = set()
                        for it in context_pack:
                            cid = it.get("chunk_id")
                            if isinstance(cid, int) and 0 <= cid < len(_meta) and cid not in seen:
                                m = _meta[cid]
                                lang = m.get("lang")
                                anchor = _make_anchor({"source": m.get("source"), "page": m.get("page")})
                                rows_ctx.append({
                                    "source": m.get("source"),
                                    "page": m.get("page"),
                                    "citation": it.get("citation"),
                                    "lang": lang,
                                    "reason_score": it.get("rerank_score"),
                                    "mmr_gain": {
                                        "rel": it.get("mmr_rel"),
                                        "div": it.get("mmr_div"),
                                        "score": it.get("mmr_score"),
                                    },
                                    "dup_flags": None,
                                    "tokens_est": _tokens_est(m.get("text"), lang),
                                    "anchor": anchor,
                                })
                                seen.add(cid)
                        st.dataframe(rows_ctx, width='stretch')

                        # Ð¡Ð²Ð¾Ð´ÐºÐ°
                        k = len(context_pack)
                        n = len(reranked)
                        budget_used = int(context_stats.get("budget_used_tokens", 0))
                        budget_limit = int(context_stats.get("budget_limit", 0))
                        lang_dist = context_stats.get("lang_distribution", {}) or {}
                        docs_dist = context_stats.get("doc_distribution", {}) or {}
                        st.markdown(f"Ð’Ð·ÑÑ‚Ð¾ {k} Ð¸Ð· {n}; Ð±ÑŽÐ´Ð¶ÐµÑ‚ {budget_used}/{budget_limit}")
                        st.markdown(f"Ð¯Ð·Ñ‹ÐºÐ¸: {lang_dist}")
                        st.markdown(f"Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸: {docs_dist}")

                        # ÐŸÐ¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¸ Ñ€ÐµÐ¶Ð¸Ð¼Ñ‹ Ð¾ÑÐ»Ð°Ð±Ð»ÐµÐ½Ð¸Ñ
                        thresholds = context_stats.get("thresholds", {}) or {}
                        with st.expander("ÐŸÐ¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¸ Ñ€ÐµÐ¶Ð¸Ð¼Ñ‹ Ð¾ÑÐ»Ð°Ð±Ð»ÐµÐ½Ð¸Ñ"):
                            st.json(thresholds)

                        # ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ñ‹ (rejected reasons)
                        rejected_reasons = context_stats.get("rejected_reasons", {}) or {}
                        with st.expander("ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ñ‹"):
                            rej_rows = [
                                {"reason": k, "count": v}
                                for k, v in sorted(rejected_reasons.items(), key=lambda x: (-int(x[1]), str(x[0])))
                            ]
                            st.dataframe(rej_rows, width='stretch')

                    # ÐžÑ‚Ð»Ð°Ð´ÐºÐ° efSearch: Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)
                    with st.expander("Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° efSearch"):
                        st.write(f"Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ efSearch (ÑÐµÑÑÐ¸Ñ): {rc.HNSW_EF_SEARCH_BASE}")
                        st.caption("ÐŸÑ€Ð¸Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ð¹ ef Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ dense-Ð²ÐµÑ‚ÐºÐ¸ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ RAG_DEBUG=1 Ð² ÑÐµÑ€Ð²ÐµÑ€Ð½Ñ‹Ð¹ Ð»Ð¾Ð³.")

                    # ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð´Ð¾Ð»Ð¸ Ð¿Ð¾ Ð²ÐµÑ‚ÐºÐ°Ð¼
                    with st.expander("Ð¯Ð·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð´Ð¾Ð»Ð¸ Ð¿Ð¾ Ð²ÐµÑ‚ÐºÐ°Ð¼"):
                        if active_branches:
                            from rag_core import chunks_metadata
                            def _share(items, target_lang):
                                if not items:
                                    return (0, 0, 0.0)
                                same = 0
                                for it in items:
                                    cid = it.get("chunk_id")
                                    if isinstance(cid, int) and 0 <= cid < len(chunks_metadata):
                                        if chunks_metadata[cid].get("lang") == target_lang:
                                            same += 1
                                other = len(items) - same
                                ratio = (same / len(items)) if len(items) else 0.0
                                return (same, other, ratio)
                        
                        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ reranked ÐºÐ°Ðº ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ top-N
                        branch_map = {
                            "dense_en": ('en', [it for it in fused if 'dense_en' in (it.get('hits') or [])]),
                            "bm25_en": ('en', [it for it in fused if 'bm25_en' in (it.get('hits') or [])]),
                            "dense_ru": ('ru', [it for it in fused if 'dense_ru' in (it.get('hits') or [])]),
                            "bm25_ru": ('ru', [it for it in fused if 'bm25_ru' in (it.get('hits') or [])]),
                        }
                        for br in active_branches:
                            target_lang, items = branch_map.get(br, (None, []))
                            if not target_lang:
                                continue
                            same, other, share = _share(items, target_lang)
                            st.write(f"{br}: same_lang={same}, other_lang={other}, share={share:.2f}")

                    # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
                    try:
                        gen_fn = st.session_state.rag_chain.get("answer_question") if isinstance(st.session_state.rag_chain, dict) else None
                        if callable(gen_fn):
                            gen_out = gen_fn(prompt, apply_lang_quota=bool(lang_filter_flag))
                        else:
                            gen_out = {"final_answer": "", "used_sources": [], "answer_lang_detected": None, "flags": {}}
                    except Exception as _e:
                        gen_out = {"final_answer": f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {_e}", "used_sources": [], "answer_lang_detected": None, "flags": {}}

                    st.subheader("ÐžÑ‚Ð²ÐµÑ‚")
                    answer_text = gen_out.get("final_answer") or ""
                    st.markdown(answer_text)

                    used_labels = gen_out.get("used_sources", []) or []
                    from rag_core import chunks_metadata as _meta
                    rows_sources = []
                    for label in used_labels:
                        cid = sources_map.get(label)
                        if isinstance(cid, int) and 0 <= int(cid) < len(_meta):
                            m = _meta[int(cid)]
                            rows_sources.append({
                                "S#": label,
                                "source": m.get("source"),
                                "page": m.get("page"),
                                "anchor": _make_anchor({"source": m.get("source"), "page": m.get("page")}),
                            })
                    with st.expander("Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²", expanded=bool(rows_sources)):
                        st.dataframe(rows_sources, width='stretch')

                    flags = gen_out.get("flags", {}) or {}
                    lang_detected = gen_out.get("answer_lang_detected")
                    st.caption(f"Ð¯Ð·Ñ‹Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {lang_detected or 'unk'} | ÐŸÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸: lang={bool(flags.get('regenerated_for_lang'))}, citations={bool(flags.get('regenerated_for_citations'))}")

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"Ð¡Ð²Ð¾Ð´ÐºÐ°: {{'fused': {len(fused)}, 'reranked': {len(reranked)}}}",
                    })
                except Exception as e:
                    st.error(f"ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ/ÑÐ»Ð¸ÑÐ½Ð¸Ð¸/Ñ€ÐµÑ€Ð°Ð½ÐºÐµ: {e}")